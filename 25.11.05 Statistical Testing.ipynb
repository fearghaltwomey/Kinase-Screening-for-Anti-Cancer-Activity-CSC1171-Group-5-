{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58661587-e146-4484-8387-c3ccd45ce03c",
   "metadata": {},
   "source": [
    "# Regression Stats Tests\n",
    "\n",
    "**Friedman Test and Nemenyi Test**\n",
    "\n",
    "Friedman is supposedly good for R^2 and RMSE/MSE, so might lean towards it. Adopts H0 of \"All models perform equally\"\n",
    "Nemenyi Post-Hoc - use after Friedman to ID which pairs of models differe (rather than as a whole). Outputs a critical difference diagram (useful for presentation)\n",
    "Alt: use repeated ANOVA, but rarely robust for cheminformatics (noisy data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217bfc3-641b-466c-b56e-34e7e1bd7eee",
   "metadata": {},
   "source": [
    "# Classification Stats Tests\n",
    "\n",
    "**Friedman Test and Nemenyi Test**\n",
    "\n",
    "Same as above, works well for ROC AUC and F1-scoring\n",
    "\n",
    "**Cochran's Q Test**\n",
    "\n",
    "Supposedly quite good for binary classification, so should be applicable (all our data is 1/0 for class). Extenction of McNemar's for >2 models\n",
    "\n",
    "**Permutative Testing**\n",
    "\n",
    "Pairwise comparisons, useful for small sample sizes (irrelevant) or unknown distributions (very relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724baef4-da6c-4790-bbc8-2d2159b2c548",
   "metadata": {},
   "source": [
    "### ***References***\n",
    "\n",
    "- \"Statistical Comparisons of Classifiers over Multiple Data Sets\", J. ML. Research, 2006\n",
    "- \"Data Mining: Practical Machine Learning Tools and Techniques\", 2011\n",
    "- J. Chem. Inf, Generally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e11180-3d1b-4b30-ae11-d7ab8a8574fd",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "*Friedman Test Statistic* - Critical value for hypothesis to be defeated\n",
    "\n",
    "*P-Value* - Critical probability for the hypothesis to be defeated\n",
    "\n",
    "*Output Matrix* - If hypothesis is defeated, comparison of scoring between each model\n",
    "\n",
    "These tests rely on the scores of the optimised validation sets. They should be tested based on (at least) two metrics, so that significant difference between **models** can be found, not just between **model outputs**. That is to say, a model can have an artificially high R^2 score which is statistically better than the other models without the model itself being statistically better. If multiple key metrics are statistically better, we can assume the model itself is better. If a significant difference between models is found (F-Test Stat > 4.00, P-Value < 0.05), the matrix of scores will be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e328c85-ed86-425d-8a72-c27909783959",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1230f1c-f651-4a9b-9087-393867a17434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f742090a-119d-4617-8418-e18dfd36a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: 10-fold CV results for 6 models\n",
    "# models: LGB, XGB, RF, SVM, MLP, CatBoost\n",
    "\n",
    "# Simulated scores: shape (folds, models) (for now, see below)\n",
    "r2_scores = np.random.rand(10, 6)\n",
    "mse_scores = np.random.rand(10, 6)\n",
    "roc_auc_scores = np.random.rand(10, 6)\n",
    "f1_scores = np.random.rand(10, 6)\n",
    "\n",
    "# Will need an import function for data from parameter tuning (validation sets)\n",
    "# Using tuned models allows us to assess the best performing model at its optimal performance\n",
    "# Some models (should) improve more drastically from tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74a39eca-8100-41ba-9479-bd40824d735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_friedman_nemenyi(metric_matrix, metric_name):\n",
    "    print(f\"\\nTesting: {metric_name}\")\n",
    "    \n",
    "    # Friedman test\n",
    "    stat, p = friedmanchisquare(*[metric_matrix[:, i] for i in range(metric_matrix.shape[1])])\n",
    "    print(f\"Friedman test statistic: {stat:.4f}, p-value: {p:.4f}\")\n",
    "    \n",
    "    if p < 0.05:\n",
    "        print(f\"Significant differences found. \\n\\nNemenyi post-hoc test for {metric_name}:\")\n",
    "        df = pd.DataFrame(metric_matrix, columns=models)\n",
    "        nemenyi = sp.posthoc_nemenyi_friedman(df)\n",
    "        print(nemenyi)\n",
    "    else:\n",
    "        print(\"No significant differences found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc51450-ac56-4d1c-b865-2b40516a2bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: R² (Regression)\n",
      "Friedman test statistic: 1.8857, p-value: 0.8647\n",
      "No significant differences found.\n",
      "\n",
      "Testing: MSE (Regression)\n",
      "Friedman test statistic: 4.8000, p-value: 0.4408\n",
      "No significant differences found.\n",
      "\n",
      "Testing: ROC AUC (Classification)\n",
      "Friedman test statistic: 5.8857, p-value: 0.3175\n",
      "No significant differences found.\n",
      "\n",
      "Testing: F1-score (Classification)\n",
      "Friedman test statistic: 0.5714, p-value: 0.9893\n",
      "No significant differences found.\n"
     ]
    }
   ],
   "source": [
    "run_friedman_nemenyi(r2_scores, \"R² (Regression)\")\n",
    "run_friedman_nemenyi(mse_scores, \"MSE (Regression)\")\n",
    "run_friedman_nemenyi(roc_auc_scores, \"ROC AUC (Classification)\")\n",
    "run_friedman_nemenyi(f1_scores, \"F1-score (Classification)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f21db602-1816-4e2a-b678-5f8922c6f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: R² (Regression)\n",
      "Friedman test statistic: 16.0857, p-value: 0.0066\n",
      "Significant differences found. \n",
      "\n",
      "Nemenyi post-hoc test for R² (Regression):\n",
      "               LGB       XGB       MLP        RF       SVM  CatBoost\n",
      "LGB       1.000000  0.845079  0.016639  0.999420  1.000000  0.427525\n",
      "XGB       0.845079  1.000000  0.326040  0.958997  0.845079  0.984591\n",
      "MLP       0.016639  0.326040  1.000000  0.046736  0.016639  0.755551\n",
      "RF        0.999420  0.958997  0.046736  1.000000  0.999420  0.650490\n",
      "SVM       1.000000  0.845079  0.016639  0.999420  1.000000  0.427525\n",
      "CatBoost  0.427525  0.984591  0.755551  0.650490  0.427525  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Import Function (It's standardised but a few lines long)\n",
    "models=  [\"LGB\", \"XGB\", \"MLP\", \"RF\", \"SVM\", \"CatBoost\"]\n",
    "def import_func(path):\n",
    "    scores = pd.read_csv(path)\n",
    "    scores = scores.to_numpy(copy= True)\n",
    "    scores = scores[:, 1:]\n",
    "    scores = scores.T\n",
    "    return scores\n",
    "# In this case, each column is a model and each row is an iteration\n",
    "r2_scores = import_func(\"JAK1ParamTuning.csv\")\n",
    "run_friedman_nemenyi(r2_scores, \"R² (Regression)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "246f549f-0b4e-460e-8a37-042644e1da73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LGB': [], 'XGB': [], 'MLP': [], 'RF': [], 'SVM': [], 'CatBoost': []}\n"
     ]
    }
   ],
   "source": [
    "# extracting ranks (WIP)\n",
    "r2 = r2_scores.T\n",
    "\n",
    "LGB = [r2[0]]\n",
    "XGB = [r2[1]] \n",
    "MLP = [r2[2]] \n",
    "RF = [r2[3]]\n",
    "SVM = [r2[4]]\n",
    "CatBoost = [r2[5]]\n",
    "\n",
    "rankings = {\n",
    "    \"LGB\": [],\n",
    "    \"XGB\": [], \n",
    "    \"MLP\": [], \n",
    "    \"RF\": [],\n",
    "    \"SVM\": [],\n",
    "    \"CatBoost\": []\n",
    "}\n",
    "del r2\n",
    "print(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbe23682-3de6-405c-bd5c-329dc8656835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57499046 0.54247067 0.07955306 0.55913691 0.56740179 0.543811  ]\n"
     ]
    }
   ],
   "source": [
    "#The average (best) r2 value for each model:\n",
    "\n",
    "avg_r2 = np.mean(r2_scores, axis=0)\n",
    "print(avg_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f8284-a3c4-49d6-8c98-9d2dce4baa38",
   "metadata": {},
   "source": [
    "Review\n",
    "\n",
    "\n",
    "Ensemble model will be more robust anyways, no use in stats testing for now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
